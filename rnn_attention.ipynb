{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jieba\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from opencc import OpenCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6756])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jieba\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from opencc import OpenCC\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # hidden_size也是embedding_dim 就是刚开始 每个单词的维度 \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size \n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size) # 跟上边的一样  embedding模型的定义 \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_2 = nn.Linear(self.max_length, 1)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size) #hidden_size=256\n",
    "\n",
    "    def forward(self,decode_input,encoder_outputs,hidden):\n",
    "        decode_input = decode_input.view(1, -1) # torch.Size([1, 256])\n",
    "        \n",
    "        input_1=decode_input.repeat(len(encoder_outputs),1)\n",
    "        \n",
    "        attn_weights =torch.tanh(self.attn(torch.cat([input_1,encoder_outputs], 1)))\n",
    "        \n",
    "        attn_weights=F.softmax(self.attn_2(attn_weights),dim=0).view(1,-1)\n",
    "        # torch.cat： torch.Size([10, 512]) 把两个[10,256]的tensor 拼接 \n",
    "        # attn_weights维度为  torch.Size([1, 10]),tensor([[0.0988, 0.0997, 0.0944, 0.1047, 0.1051, 0.0925, 0.1062, 0.1043, 0.0892,0.1052]], device='cuda:0', grad_fn=<ViewBackward>)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))  # unsqueeze(0)在第一个维度上 增加一维 1 \n",
    "        # torch.bmm 三维矩阵相乘  \n",
    "        # attn_applied=torch.Size([1, 1, 256])=torch.bmm([1,1,10],[1,10,256]) \n",
    "        # print(attn_applied.size())\n",
    "        \n",
    "        output = torch.cat([decode_input , attn_applied[0]], 1)\n",
    "        # output.size():torch.Size([1, 512])\n",
    "        \n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # output.size():torch.Size([1, 1, 256])\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        #output.size():torch.Size([1, 1, 256])\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden) # 输入端hidden:torch.Size([1, 1, 256])\n",
    "        # output.size()：torch.Size([1, 1, 256])，输出端hidden.size()：torch.Size([1, 1, 256]) \n",
    "        \n",
    "        output_last = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        #output.size()：torch.Size([1, 6756])\n",
    "        \n",
    "        return output_last,output, hidden, attn_weights\n",
    "    \n",
    "attentionRNN_decoder = AttnDecoderRNN(256,6756,10)\n",
    "\n",
    "decode_input=torch.Tensor(1, 256)\n",
    "encoder_outputs=torch.Tensor(10, 256)\n",
    "hidden=torch.Tensor(1, 1, 256)\n",
    "output_last,output, hidden, attn_weights=attentionRNN_decoder(decode_input,encoder_outputs,hidden)\n",
    "\n",
    "print(output_last.size())\n",
    "print(output.size())\n",
    "print(hidden.size())\n",
    "print(attn_weights.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "D = torch.Tensor(2,3)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'D:\\\\b\\\\b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-37d641248a7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m    \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'D:\\b\\b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m    \u001b[0moutpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/c'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m    \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-37d641248a7c>\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(path, outpath)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m    \u001b[0mdirfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mdirf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'D:\\\\b\\\\b'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "def copyfile(path, outpath):\n",
    "   dirfile = os.listdir(path)\n",
    "   print((dirfile))\n",
    "   for dirf in dirfile:\n",
    "      i = 0\n",
    "      for filename in dirf:\n",
    "         i += 1\n",
    "         file1 = str(int(filename) + i)\n",
    "         newdir = path + '/' + dirf + '/' + file1 + '.jpg'\n",
    "         # if not os.path.exists(outpath):\n",
    "         #    os.makedirs(outpath)  #注意还有重命名的过程\n",
    "         out = outpath + '/' + dirf + file1 + '.jpg'\n",
    "         #shutil.copyfile(newdir, out)\n",
    "         #i += 1\n",
    "         print(('转换结束'))\n",
    "if __name__ == \"__main__\":\n",
    "   path = r'D:\\b\\b'\n",
    "   outpath = 'D:/c'\n",
    "   copyfile(path, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原文件夹\n",
    "old_path = \"G:/MP4\"\n",
    "# 查看原文件夹下所有的子文件夹\n",
    "filenames = os.listdir(old_path)\n",
    "# 新文件夹\n",
    "target_path = \"G:/MP5\"\n",
    "if not os.path.exists(target_path):\n",
    "    os.mkdir(target_path)\n",
    "\n",
    "for file in filenames:\n",
    "    # 所有的子文件夹\n",
    "    sonDir = \"G:/MP4/\" + file\n",
    "    # 遍历子文件夹中所有的文件\n",
    "    for root, dirs, files in os.walk(sonDir):\n",
    "     # 如果文件夹中有文件\n",
    "        if len(files) > 0:\n",
    "            for f in files:\n",
    "                newDir = sonDir + '/' + f\n",
    "                # 将文件移动到新文件夹中\n",
    "                shutil.move(newDir, target_path)\n",
    "        else:\n",
    "            print(sonDir + \"文件夹是空的\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "import time\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class neuralNetwork:\n",
    "    \"\"\"\n",
    "    初始化\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, innodes, hinodes, outnodes, learn):\n",
    "        self.innodes = innodes\n",
    "        self.hinodes = hinodes\n",
    "        self.outnodes = outnodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.innodes, -0.5), (self.hinodes, self.innodes))  # (100,784)\n",
    "        self.who = np.random.normal(0.0, pow(self.hinodes, -0.5), (self.outnodes, self.hinodes))  # (10,100)\n",
    "        self.lr = learn\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "\n",
    "    '''\n",
    "     以图片形式展示数据集中的数字\n",
    "    '''\n",
    "\n",
    "    def show_num(self, image_array):\n",
    "        image_array = image_array.reshape((28, 28))\n",
    "        plt.imshow(image_array, cmap='Greys', interpolation='None')\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "    数据集处理：由于使用sigmoid函数，将目标值设置到 0.01--1的范围内 并设置目标值集合\n",
    "    '''\n",
    "\n",
    "    def data_deal(self, data_path):\n",
    "        data_train = open(data_path, 'r')\n",
    "        data_list = data_train.readlines()\n",
    "        data_train.close()\n",
    "        train_list = []\n",
    "        target_list = []\n",
    "        num_list = []\n",
    "        out_nodes = 10\n",
    "        for value in data_list:\n",
    "            train_value = value.split(',')\n",
    "            train_list.append(np.asfarray(train_value[1:]) / 255.0 * 0.99 + 0.01)\n",
    "            num_list.append(np.asfarray(train_value[0]))\n",
    "            targets = np.zeros(out_nodes) + 0.01\n",
    "            targets[int(train_value[0])] = 0.99\n",
    "            target_list.append(targets)\n",
    "        return np.array(target_list), np.array(train_list), np.array(num_list)\n",
    "\n",
    "    '''\n",
    "    训练函数：前向传播计算输出，同时反向传播更新权重矩阵\n",
    "    使用的激活函数为sigmoid\n",
    "    '''\n",
    "\n",
    "    def train(self, train_set, target_set):\n",
    "        input = np.array(train_set, ndmin=2).T\n",
    "        target = np.array(target_set, ndmin=2).T\n",
    "\n",
    "        hidden_input = np.dot(self.wih, input)\n",
    "        hidden_output = self.activation_function(hidden_input)\n",
    "        final_input = np.dot(self.who, hidden_output)\n",
    "        final_output = self.activation_function(final_input)\n",
    "\n",
    "        output_error = target - final_output\n",
    "        hidden_error = np.dot(self.who.T, output_error)\n",
    "\n",
    "        self.who += self.lr * np.dot((output_error * final_output * (1.0 - final_output)), np.transpose(hidden_output))\n",
    "        self.wih += self.lr * np.dot((hidden_error * hidden_output * (1.0 - hidden_output)), np.transpose(input))\n",
    "\n",
    "    '''\n",
    "    使用train得到的权重矩阵计算最终输出值，用以和真实值进行对比，从而计算该神经网络的正确率\n",
    "    '''\n",
    "\n",
    "    def query(self, input_list):\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs\n",
    "\n",
    "\n",
    "# 初始化\n",
    "neural = neuralNetwork(784, 200, 10, 0.1)\n",
    "\n",
    "'''\n",
    "# 一开始使用100个数据的数据集进行训练，训练5次，虽然训练时的准确率挺高的 99%左右，但是在使用测试集进行测试时，准确率降到了60%\n",
    "#推测为训练数据集中的数据不够随机，导致测试时准确度下降\n",
    "target, train, num = neural.data_deal(\"dataSet/mnist_train_100.csv\")\n",
    "for record in range(5):\n",
    "    score = []\n",
    "    for i, j, n in zip(train, target, num):\n",
    "        neural.train(i, j)\n",
    "        outputs = neural.query(i)\n",
    "        if n == np.argmax(outputs):\n",
    "            score.append(n)\n",
    "print(float(len(score) / len(num)))\n",
    "'''\n",
    "# 改用包含60000个数据的数据集进行训练，准确度提升至98% 且测试时的准确度在97%左右\n",
    "\n",
    "target, train, num = neural.data_deal(\"dataSet/mnist_train_100.csv\")\n",
    "start_time = time.time()\n",
    "for a in range(1000):\n",
    "    score = []\n",
    "    for i, j, n in zip(train, target, num):\n",
    "        neural.train(i, j)\n",
    "        outputs = neural.query(i)\n",
    "\n",
    "        if n == np.argmax(outputs):\n",
    "            score.append(n)\n",
    "    if a % 100 == 0:\n",
    "        print(float(len(score) / len(num)))\n",
    "end_time = time.time()\n",
    "print(\"消耗时间：\", str(end_time - start_time))\n",
    "# 由于测试集数据比较少，因此改用含有100个数据的训练集进行测试\n",
    "# test_target, test_train, test_num = neural.data_deal(\"dataSet/mnist_test_10.csv\")\n",
    "# test_score = []\n",
    "# num_list = []\n",
    "# for o, p, q in zip(test_train, test_target, test_num):\n",
    "#     outputs = neural.query(o)\n",
    "#     num_list.append(np.argmax(outputs))\n",
    "#     if q == np.argmax(outputs):\n",
    "#         test_score.append(q)\n",
    "#\n",
    "# print(float(len(test_score) / len(test_num)))\n",
    "# print(num_list)\n",
    "# print(test_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "torch.Size([10, 10])\n",
      "tensor([[0.0986],\n",
      "        [0.1090],\n",
      "        [0.0911],\n",
      "        [0.0951],\n",
      "        [0.1071],\n",
      "        [0.1038],\n",
      "        [0.1023],\n",
      "        [0.0983],\n",
      "        [0.0978],\n",
      "        [0.0970]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(1,256)\n",
    "a=a.repeat(10,1)\n",
    "print(a.size())\n",
    "b=torch.rand(10,256)\n",
    "attn = nn.Linear(512, 10)\n",
    "weights = torch.tanh(attn(torch.cat([a, b],1)))\n",
    "print(weights.size())\n",
    "v=nn.Linear(10,1)\n",
    "attention = F.softmax(v(weights),dim=0)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: torch.Size([1, 17, 400]) tensor([[[0.6346, 0.0408, 2.2800,  ..., 0.5100, 0.5529, 0.4370],\n",
      "         [0.6346, 0.0408, 2.2800,  ..., 0.5100, 0.5529, 0.4370],\n",
      "         [0.6346, 0.0408, 2.2800,  ..., 0.5100, 0.5529, 0.4370],\n",
      "         ...,\n",
      "         [0.2948, 0.1023, 2.2115,  ..., 0.1425, 0.4066, 0.5442],\n",
      "         [0.2948, 0.1023, 2.2115,  ..., 0.1425, 0.4066, 0.5442],\n",
      "         [0.2948, 0.1023, 2.2115,  ..., 0.1425, 0.4066, 0.5442]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "attention: torch.Size([4, 17, 17]) tensor([[[0.0326, 0.0494, 0.0941,  ..., 0.0941, 0.0658, 0.0326],\n",
      "         [0.0721, 0.0342, 0.0467,  ..., 0.0467, 0.0789, 0.0721],\n",
      "         [0.0332, 0.0739, 0.0409,  ..., 0.0409, 0.0937, 0.0332],\n",
      "         ...,\n",
      "         [0.0332, 0.0739, 0.0409,  ..., 0.0409, 0.0937, 0.0332],\n",
      "         [0.0489, 0.0234, 0.0311,  ..., 0.0311, 0.1344, 0.0489],\n",
      "         [0.0326, 0.0494, 0.0941,  ..., 0.0941, 0.0658, 0.0326]],\n",
      "\n",
      "        [[0.0356, 0.0486, 0.0820,  ..., 0.0820, 0.0749, 0.0356],\n",
      "         [0.0710, 0.0393, 0.0900,  ..., 0.0900, 0.0319, 0.0710],\n",
      "         [0.0240, 0.0319, 0.1379,  ..., 0.1379, 0.0502, 0.0240],\n",
      "         ...,\n",
      "         [0.0240, 0.0319, 0.1379,  ..., 0.1379, 0.0502, 0.0240],\n",
      "         [0.0486, 0.0926, 0.0647,  ..., 0.0647, 0.0320, 0.0486],\n",
      "         [0.0356, 0.0486, 0.0820,  ..., 0.0820, 0.0749, 0.0356]],\n",
      "\n",
      "        [[0.0406, 0.0930, 0.0329,  ..., 0.0329, 0.0734, 0.0406],\n",
      "         [0.0317, 0.1368, 0.0498,  ..., 0.0498, 0.0238, 0.0317],\n",
      "         [0.0887, 0.0620, 0.0307,  ..., 0.0307, 0.0465, 0.0887],\n",
      "         ...,\n",
      "         [0.0887, 0.0620, 0.0307,  ..., 0.0307, 0.0465, 0.0887],\n",
      "         [0.0479, 0.0810, 0.0740,  ..., 0.0740, 0.0351, 0.0479],\n",
      "         [0.0406, 0.0930, 0.0329,  ..., 0.0329, 0.0734, 0.0406]],\n",
      "\n",
      "        [[0.1238, 0.0450, 0.0215,  ..., 0.0215, 0.0287, 0.1238],\n",
      "         [0.0637, 0.0315, 0.0478,  ..., 0.0478, 0.0911, 0.0637],\n",
      "         [0.0784, 0.0716, 0.0340,  ..., 0.0340, 0.0464, 0.0784],\n",
      "         ...,\n",
      "         [0.0784, 0.0716, 0.0340,  ..., 0.0340, 0.0464, 0.0784],\n",
      "         [0.0883, 0.0313, 0.0697,  ..., 0.0697, 0.0386, 0.0883],\n",
      "         [0.1238, 0.0450, 0.0215,  ..., 0.0215, 0.0287, 0.1238]]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class dot_attention(nn.Module):\n",
    "    \"\"\" 点积注意力机制\"\"\"\n",
    "\n",
    "    def __init__(self, attention_dropout=0.0):\n",
    "        super(dot_attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param q:\n",
    "        :param k:\n",
    "        :param v:\n",
    "        :param scale:\n",
    "        :param attn_mask:\n",
    "        :return: 上下文张量和attention张量。\n",
    "        \"\"\"\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))\n",
    "        if scale:\n",
    "            attention = attention * scale        # 是否设置缩放\n",
    "        if attn_mask:\n",
    "            attention = attention.masked_fill(attn_mask, -np.inf)     # 给需要mask的地方设置一个负无穷。\n",
    "        # 计算softmax\n",
    "        attention = self.softmax(attention)\n",
    "        # 添加dropout\n",
    "        attention = self.dropout(attention)\n",
    "        # 和v做点积。\n",
    "        context = torch.bmm(attention, v)\n",
    "        return context, attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" 多头自注意力\"\"\"\n",
    "    def __init__(self, model_dim=400, num_heads=4, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim_per_head = model_dim//num_heads   # 每个头的维度\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
    "\n",
    "        self.dot_product_attention = dot_attention(dropout)\n",
    "\n",
    "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)         # LayerNorm 归一化。\n",
    "\n",
    "    def forward(self, key, value, query, attn_mask=None):\n",
    "        # 残差连接\n",
    "        residual = query\n",
    "\n",
    "        dim_per_head = self.dim_per_head\n",
    "        num_heads = self.num_heads\n",
    "        batch_size = key.size(0)\n",
    "\n",
    "        # 线性映射。\n",
    "        key = self.linear_k(key)\n",
    "        value = self.linear_v(value)\n",
    "        query = self.linear_q(query)\n",
    "\n",
    "        # 按照头进行分割\n",
    "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
    "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
    "\n",
    "        if attn_mask:\n",
    "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "\n",
    "        # 缩放点击注意力机制\n",
    "        scale = (key.size(-1) // num_heads) ** -0.5\n",
    "        context, attention = self.dot_product_attention(query, key, value, scale, attn_mask)\n",
    "\n",
    "        # 进行头合并 concat heads\n",
    "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
    "\n",
    "        # 进行线性映射\n",
    "        output = self.linear_final(context)\n",
    "\n",
    "        # dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # 添加残差层和正则化层。\n",
    "        output = self.layer_norm(residual + output)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = torch.ones((1, 17, 400))\n",
    "    k = torch.ones((1, 17, 400))\n",
    "    v = k\n",
    "    mutil_head_attention = MultiHeadAttention()\n",
    "    output, attention = mutil_head_attention(q, k, v)\n",
    "    print(\"context:\", output.size(), output)\n",
    "    print(\"attention:\", attention.size(), attention)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
